# 贝叶斯学派与频率学派
举个例子来讲抛硬币实验。
频率学派认为其正面概率可以通过统计得到，也就是说世界是确定的，假设抛N次硬币有m个正面那么：
$$
p(\theta)=\frac{m}{N}
$$
而贝叶斯学派则认为，概率本身也是有先验分布的，由于我们对硬币一无所知，因此假设正面概率为均匀分布：

$$
\begin{matrix}
\theta \thicksim U(0, 1)\\
p(\theta)=1
\end{matrix}
$$

注意这里为硬币取正面的概率，这个概率我们认为是一个0-1区间均匀分布的。这代表我们未对数据引入任何先验知识。但是我们知道抛了N次硬币有m个正面，此时可以对theta进行估计：

$$
p(\theta|m)=\frac{p(m|\theta)p(\theta)}{p(m)}
=\frac{p(m|\theta)p(\theta)}{\int{p(m|\phi)d\phi}}
$$

由此可得：

$$
\begin{matrix}
p(\theta)=1\\
p(m|\theta)=C_N^{m}\theta^m(1-\theta)^{N-m}\\
\int(m|\phi)d\phi=\int C_N^{m}\phi^m(1-\phi)^{N-m}
\end{matrix}
$$

此时$$\theta$$分布为Beta分布，对于m=N-m也就是正反数据均等的情况下，theta最大值为0.5，这就是硬币取得正面的概率。这也就是说：

> 先验概率+数据=后验概率

- $p(\theta|m)$为**后验**
- $p(m|\theta)$为**似然**
- $p(\theta)$为**先验**

### TIPS：独立同分布=Independent and identical distribution=i.i.d.

# 最大似然估计(MLE)

取合适的$\theta$使得似然最大：

$$
\begin{matrix}
\hat{\theta}=\mathop{\arg\max}_{\theta}p(y|\theta)\\
=\mathop{\arg\max}_{\theta}\Pi_i p(y_i|\theta)\\
=\mathop{\arg\max}_{\theta}log(\Pi_i p(y_i|\theta))\\
=\mathop{\arg\min}_{\theta}-\sum log(y_i|\theta)
\end{matrix}
$$

对于多分类问题而言，按照每一类概率进行统计：

$$
\begin{matrix}
\mathop{\arg\min}_{\theta}-\sum log(y_i|\theta)/N\\
=\mathop{\arg\min}_{\theta}-p_i\sum log(y_i|\theta)
=CrossEntropy
\end{matrix}
$$

实际上交叉熵即是最大似然估计。

# 最大后验估计MAP
$$
\begin{matrix}
\hat{\theta}=\mathop{\arg\max}_{\theta}p(\theta|y)\\
=\mathop{\arg\min}_{\theta}-\sum log(y_i|\theta)-log (p(\theta))
\end{matrix}
$$

假设先验分布为高斯分布，此时：

$$
log (p(\theta))=c+\frac{\theta^2}{2\sigma^2}
$$

相当于在最大似然估计的基础上加入了正则化项目。

#### TIPS本身概率的概率引入就是为了引入先验，而防止数据偏差造成结果的偏差。因此最大后验估计必然会有正则化项以防止过拟合问题。





# EM算法
假设观测数据$x=(x_1, x_2, x_3,\cdots)$，其对数似然函数为：

$$
L=\sum_i log(p(x_i|\theta))
$$1

假设数据中含有隐藏变量$z=(z_1, z_2,\cdots)$

其变量变为：

$$
L=\sum_i log(\sum_j p(x_i, z_j|\theta))
$$2

假设z的分布为$Q(z)$，则：

$$
L=\sum_i log(\sum_j Q(z) \frac{p(x_i, z_j|\theta)}{Q(z_j)})
$$3

Jensen不等式，假设f为凹函数：

$$
\mathbb{E}(f(x))\le f(\mathbb{E}(x))
$$4
当且仅当f(x)为常数时等号成立。

$$
L = \sum_i \sum_j Q(z_j)log(\frac{p(x_i, z_j|\theta)}{Q(z_j)})
$$5

等式成立时

$$
\begin{matrix}
\frac{p(x_i, z_j|\theta)}{Q(z_j)}=c\\
\sum_j Q(z_i)=1\\
\rightarrow
\sum_j p(x_i, z_j|\theta)=p(x_i|\theta)=c\\
\rightarrow 
Q(z_j)=\frac{p(x_i, z_j|\theta)}{p(x_i|\theta)}\\
=p(z_j|x_i,\theta)
\end{matrix}
$$6

3式确定了L的下界，此时根据6可以求变量z的分布Q(z)

所以EM算法的步骤为：

E步：
根据theta和数据估计变量z的分布：
$$
Q(z)=p(z|x,\theta)
$$7

M步:
使theta最大：

$$
\theta \leftarrow \mathop{\arg\max}_{\theta} \sum_i \sum_j  Q(z_j) log(\frac{p(x_i, z_j|\theta)}{Q(z_j)})
$$8

8式下界即为5式。

## 例子
硬币，假设有三个硬币ABC，先抛A，如果是正面则选B，否则选C，抛掷选择的硬币，出现正面记为1,否则记为0，假设三个硬币正面概率为a,b,c。独立进行n次试验记录样本x。仅记录最终的01。

此时隐变量z为选择硬币B。

根据上面所说，
E步-隐藏变量分布
EM算法第一步E步，为观测隐藏变量分布。在假设的abc条件以及观测变量$x_i$的条件下计算抛硬币为B的概率：

$$
\mu_i =p(B|x_i,\theta)=\frac{p(x_i|B, \theta)}{p(x_i|B, \theta)+p(x_i|\urcorner B, \theta)}=\frac{a b^{x_i}(1-b)^{1-
x_i}}{a b^{x_i}(1-b)^{1-
x_i}+(1-a)c^{x_i}(1-c)^{1-
x_i}}
$$

M步-参数最大化

$$
\begin{matrix}
a = \frac{1}{n}\sum_i \mu_i\\
b = \frac{\sum_i \mu_i y_i}{\sum_i{\mu_i}}\\
c = \frac{\sum_i (1-\mu_i) y_i}{\sum_i{(1-\mu_i)}}\\
\end{matrix}
$$

具体推演过程参《统计学习方法》李航

代码coin.py